# STT Corrector - RAG System

Sistem koreksi kesalahan Speech-to-Text (STT) berbasis **knowledge base + embedding + vector search (ChromaDB)**,
dengan opsi tambahan **LLM (Ollama / llama.cpp)** untuk normalisasi teks berbasis konteks.

## ğŸ¯ Fitur

- **Koreksi STT Otomatis**: Mengoreksi kesalahan pengenalan suara berdasarkan knowledge base
- **RAG Pipeline**: Menggunakan embedding `sentence-transformers` + vector similarity search di ChromaDB
- **Knowledge Base Dinamis**: Tambahkan koreksi baru melalui API
- **Backend-only REST API**: Mudah diintegrasikan ke pipeline STT / aplikasi lain
- **LLM Opsional**: Support **Ollama** atau **llama.cpp** untuk normalisasi teks

## ğŸ“ Struktur Proyek

```
corector/
â”œâ”€â”€ config.py              # Konfigurasi sistem (embedding, Chroma, server)
â”œâ”€â”€ main.py                # Entry point aplikasi (menjalankan FastAPI)
â”œâ”€â”€ requirements.txt       # Dependencies Python
â”œâ”€â”€ .env                   # Environment variables (HOST, PORT, EMBEDDING_MODEL, dst.)
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ knowledge_base.json    # Knowledge base koreksi (correct_phrase + common_mistakes)
â”‚   â””â”€â”€ chroma_db/             # Vector database (auto-generated oleh Chroma)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api.py             # FastAPI endpoints (REST API backend-only)
â”‚   â”œâ”€â”€ corrector.py       # Main corrector logic (n-gram + vector store)
â”‚   â”œâ”€â”€ embeddings.py      # Embedding model (SentenceTransformers)
â”‚   â””â”€â”€ vector_store.py    # ChromaDB vector store wrapper
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ init_db.py         # Initialize database dari knowledge_base.json (opsional)
    â””â”€â”€ test_corrector.py  # Test script (opsional)
```

## ğŸš€ Instalasi & Menjalankan

### 1. Clone Repo

```bash
git clone https://github.com/ArifMunawarr/corectorrag.git
cd corectorrag
```

### 2. Buat Virtualenv & Install Dependencies

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 3. Konfigurasi Dasar

Buat file `.env` di root proyek (jika belum ada), misalnya:

```env
# Embedding model
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# ChromaDB
CHROMA_PERSIST_DIR=./data/chroma_db

# Server
HOST=0.0.0.0
PORT=8888
```

Sesuaikan `PORT` jika perlu.

### 4. Jalankan Server Secara Manual

```bash
source venv/bin/activate
python main.py
```

Secara default server akan berjalan di `http://0.0.0.0:PORT` (misal `http://localhost:8888`).

## ğŸ“– Penggunaan

### REST API

Untuk mengaktifkan LLM (RAG + LLM normalizer), set `use_llm` ke `true`:

```bash
curl -X POST http://localhost:8888/correct \
  -H "Content-Type: application/json" \
  -d '{"text": "beso kit mulai pelatihan nek ji"}'
```

#### Koreksi Teks (output sederhana)

Endpoint khusus yang hanya mengembalikan teks koreksi:

```bash
curl -X POST http://localhost:8888/correct/plain \
  -H "Content-Type: application/json" \
  -d '{"text": "start eating"}'
```

Response:

```json
{ "corrected_text": "start meeting" }
```

#### Tambah Koreksi Baru

```bash
curl -X POST http://localhost:8888/knowledge/add \
  -H "Content-Type: application/json" \
  -d '{
    "correct_phrase": "book appointment",
    "common_mistakes": ["book a point meant", "book a pointment"],
    "context": "Membuat janji",
    "category": "scheduling"
  }'
```

#### Cek Status

```bash
curl http://localhost:8888/stats
```

## ğŸ”§ Konfigurasi

### Environment Variables (`.env`)

```env
# Embedding model
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# ChromaDB
CHROMA_PERSIST_DIR=./data/chroma_db

# Server
HOST=0.0.0.0
PORT=8888
```

### LLM Backend (`config.py`)

Edit `config.py` untuk memilih backend LLM:

#### OPSI 1: Ollama (default)

```python
LLM_BACKEND: str = os.getenv("LLM_BACKEND", "ollama")
OLLAMA_BASE_URL: str = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
LLM_MODEL: str = os.getenv("LLM_MODEL", "llama3.2:latest")
```

Jalankan Ollama:
```bash
ollama run llama3.2:latest
```

#### OPSI 2: llama.cpp

Comment OPSI 1, uncomment OPSI 2 di `config.py`:

```python
LLM_BACKEND: str = os.getenv("LLM_BACKEND", "llama_cpp")
LLAMA_CPP_URL: str = os.getenv("LLAMA_CPP_URL", "http://localhost:8080")
```

Jalankan llama.cpp server:
```bash
./llama-server -m /path/to/model.gguf --port 8080
```

#### Tanpa LLM

Jika tidak ingin pakai LLM, cukup set `use_llm: false` di request API.

## ğŸ“ Knowledge Base

Edit `data/knowledge_base.json` untuk menambah/mengubah koreksi:

```json
{
  "corrections": [
    {
      "correct_phrase": "start meeting",
      "common_mistakes": ["start eating", "start meting"],
      "context": "Memulai rapat",
      "category": "meeting"
    }
  ]
}
```

## ğŸ› ï¸ Tech Stack

- **Embeddings**: SentenceTransformers (`paraphrase-multilingual-MiniLM-L12-v2`)
- **Vector Store**: ChromaDB
- **Backend**: FastAPI + Uvicorn
- **LLM**: Ollama atau llama.cpp (opsional)